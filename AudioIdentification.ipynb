{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joe Zacaroli - Audio Identification Assignment\n",
    "\n",
    "  \n",
    "\n",
    "You will first need to run the first 8 cells to create all the required functions.\n",
    "\n",
    "I would not recommend running the cells preceded with 'Not intended for marker use' because they either aren't guaranteed to work or they may just take a long time to run.\n",
    "\n",
    "Then there are some cells at the bottom preceded with 'Functions and cells required for assignment'. These are what you will need to run to create the desired output text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.feature import peak_local_max\n",
    "from scipy.stats import mode\n",
    "from random import randrange\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "#To use line_profiler: 'pip install line_profiler' and uncomment the next line.\n",
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFingerprint(fileName, params, plotSpectrograms=False):\n",
    "    \"\"\"Fingerprint is computed by either (a) finding peaks in the melspectrogram using the inverted list method.\n",
    "                                         (b) finding pairs of peaks in the melspectrogram.\n",
    "    \"\"\"\n",
    "    snd, sr = librosa.load(fileName)\n",
    "    lengthOfFile_seconds=len(snd)//sr\n",
    "    \n",
    "    #Get all of the variables out of the params\n",
    "    usePeakPairs = params['usePeakPairs']\n",
    "    hopLength = params['hopLength']\n",
    "    nFFT = params['nFFT']\n",
    "    maxPeaksPerSecond = params['maxPeaksPerSecond']\n",
    "    peakDetectionMinDistance = params['peakDetectionMinDistance']\n",
    "    fMax = params['fMax']\n",
    "    fanOut = params['fanOut']\n",
    "    targetZoneK = params['targetZoneK']\n",
    "    targetZoneT = params['targetZoneT']\n",
    "    \n",
    "    useSTFT = True\n",
    "    if useSTFT:\n",
    "        S = np.abs(librosa.core.stft(snd, n_fft=nFFT, hop_length=hopLength))\n",
    "    else:\n",
    "        S = librosa.feature.melspectrogram(snd, sr=sr, hop_length=hopLength, n_mels=nFFT, fmax=fMax)\n",
    "\n",
    "    #using skimage's peak detection function.\n",
    "    coordinates = peak_local_max(S, min_distance=peakDetectionMinDistance, num_peaks=maxPeaksPerSecond*lengthOfFile_seconds)\n",
    "    #sort them by their time step\n",
    "    coordinates = coordinates[np.argsort(coordinates[:, 1])]\n",
    "    \n",
    "    if plotSpectrograms:\n",
    "        if useSTFT:\n",
    "            librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max), y_axis='log', x_axis='time')\n",
    "            plt.title('Power spectrogram')\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            #Find frequencies of the mel bins I'm using.\n",
    "            mel_freq_bins = librosa.core.mel_frequencies(n_mels=n_hashes, fmax=fMax)\n",
    "\n",
    "            S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "            plt.figure(figsize=(10, 2))\n",
    "            plt.title('Mel-frequency spectrogram: ' + fileName)\n",
    "            librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=fMax)\n",
    "            plt.figure(figsize=(10, 2))\n",
    "            for coordinate in coordinates:\n",
    "                plt.plot(coordinate[1]*hopLength/sr, mel_freq_bins[coordinate[0]], 'wo')\n",
    "            librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=fMax)\n",
    "    \n",
    "    hashLists = constructHashLists(coordinates, nFFT, usePeakPairs, fanOut=fanOut, targetZoneK=targetZoneK, targetZoneT=targetZoneT)\n",
    "    \n",
    "    return hashLists\n",
    "\n",
    "def constructHashLists(peakCoordinates, n_hashes, usePeakPairs, fanOut=10, targetZoneK=20, targetZoneT=30):\n",
    "    \"\"\"Creates a list for each hash (frequency band) which contains the\n",
    "    points at which that frequency band has a peak in the audio.\n",
    "    \n",
    "    Inputs: peakCoordinates - np array of the form:\n",
    "            [[hashKey, timeStep],\n",
    "            [hashKey, timeStep]\n",
    "                      ..\n",
    "                      ..\n",
    "            [hashKey, timeStep]]\n",
    "            \n",
    "            n_hashes - the number of hash lists to create.\n",
    "            \n",
    "            usePeakPairs (Bool) - defines whether or not to use the inverted list or peak pair methods.\n",
    "            fanOut, targetZoneK, targetZoneT are used and defined in the peak pair method.\n",
    "    \n",
    "    Outputs: hash_lists. Dictionary of the form: {hash_number, hash_list}\n",
    "                        where hash_number is just a frequency bin number and hash_list is a list of points where that frequency has a peak\"\"\"\n",
    "    \n",
    "    if usePeakPairs:\n",
    "        return constructPeakPairHashLists(peakCoordinates, n_hashes, fanOut=fanOut, targetZoneK=targetZoneK, targetZoneT=targetZoneT)\n",
    "    \n",
    "    hash_lists = {}\n",
    "    for hashNum in range(n_hashes):\n",
    "        hash_list = peakCoordinates[np.nonzero(peakCoordinates[:,0]==hashNum),1]\n",
    "        if np.size(hash_list)>0:\n",
    "            hash_lists[hashNum] = hash_list[0]\n",
    "    \n",
    "    return hash_lists\n",
    "\n",
    "def constructPeakPairHashLists(peakCoordinates, n_hashes, fanOut=10, targetZoneK=20, targetZoneT=30):\n",
    "    \"\"\"\n",
    "    For each anchor peak in peakCoordinates, find other peaks within a target zone, up to a maximum of fanOut.\n",
    "    Inputs: peakCoordinates - as defined above\n",
    "            fanOut - The maximum amount of hashed points to return for each peak\n",
    "            targetZoneK - Half the target zone frequency size\n",
    "            targetZoneT - The target zone time size\n",
    "    Returns: hash_list. Dictionary of the form: {hash_key, time_offset}\n",
    "                        where hash_key is a tuple of (k1, k2, t2-t1)]\"\"\"\n",
    "    hash_list = {}\n",
    "    \n",
    "    for anchorPeak in peakCoordinates:\n",
    "        numberOfHashesAddedForThisPeak=0\n",
    "        for otherPeak in peakCoordinates:\n",
    "            #Check that it's within the time range of the target zone.\n",
    "            if anchorPeak[1]+15 < otherPeak[1] <= anchorPeak[1] + targetZoneT+15:\n",
    "                #Check that it's within the frequency range of the target zone.\n",
    "                if anchorPeak[0]-targetZoneK <= otherPeak[0] <= anchorPeak[0]+targetZoneK:\n",
    "                    #Found another peak in the target zone! Add it to the hash_list\n",
    "                    hash_list[(anchorPeak[0], otherPeak[0], otherPeak[1]-anchorPeak[1])] = anchorPeak[1]\n",
    "                    numberOfHashesAddedForThisPeak += 1\n",
    "            \n",
    "            if numberOfHashesAddedForThisPeak >= fanOut:\n",
    "                break\n",
    "    return hash_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingAverage(x, kernel_size=3):\n",
    "    \"\"\" \n",
    "    Moving average of a numpy array x\n",
    "    kernel_size is the size of the moving average to be applied. Should be an odd number!\n",
    "    \"\"\"\n",
    "    ret_val = np.zeros(len(x)-kernel_size+1)\n",
    "    assert(kernel_size&0x1)\n",
    "    \n",
    "    for i in range(kernel_size//2, len(x)-(kernel_size//2)):\n",
    "        ret_val[i-1-kernel_size//2] = sum(x[i-(kernel_size//2):1+i+(kernel_size//2)])/kernel_size\n",
    "\n",
    "    return ret_val\n",
    "\n",
    "X = movingAverage(np.array([1, 1, 1, 1, 1, 1, 1, 1, 1]), kernel_size=7)\n",
    "assert((X==[1,1,1]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMatchingFunction(queryHashList, databaseHashList, usePeakPairs=False, plotStuff=False):\n",
    "    \"\"\"Computes the matching function for a query fingerprint given a database fingerprint.\n",
    "    \n",
    "    If usePeakPairs is false then fingerprints are of the form: {hashNumber: [hash_list], hashNumber: [hash_list], .. , hashNumber: [hash_list]}\n",
    "                                and the inverted list technique is used\n",
    "    \n",
    "    If usePeakPairs is true then fingerprints are of the form: {(hashTuple):timeOffset, (hashTuple):timeOffset, .. , (hashTuple):timeOffset}\n",
    "                                and the technique proposed by Wang (2003) is used.\"\"\"\n",
    "    \n",
    "    if usePeakPairs:\n",
    "        return computePairMatchingFunction(queryHashList, databaseHashList)\n",
    "    \n",
    "    #For each query point, We take away the time step value from the associated hash list.\n",
    "    #Then work out what number appears in the indicator functions the most times.\n",
    "    indicatorFunctions = np.empty((1,0), int)\n",
    "    for h in queryHashList:\n",
    "        for n in queryHashList[h]:\n",
    "            #There is a peak defined by (n,h)\n",
    "            #Only look up if there exists a hash list in the database recording for this frequency bin h.\n",
    "            if (h in databaseHashList):\n",
    "                #Take away the time step value for this query point from every time point in the database's hash list.\n",
    "                indicatorFunctions = np.append(indicatorFunctions, databaseHashList[h] - n)\n",
    "    \n",
    "    #It's possible that none of the hash lists of the query line up with the hash lists of the recording.\n",
    "    #If this is the case, return 0 as there are zero matches\n",
    "    if np.size(indicatorFunctions)==0:\n",
    "        return 0\n",
    "    \n",
    "    # EXPERIMENTING WITH SMOOTHING: Returning the max of the smoothed histogram.\n",
    "    # Uncomment the following 5 lines to use it.\n",
    "    #minVal = min(indicatorFunctions)\n",
    "    #maxVal = max(indicatorFunctions)\n",
    "    #indicatorHistVals = np.histogram(indicatorFunctions, bins=np.arange(minVal, maxVal))[0]\n",
    "    #indicatorHistValsSmoothed = movingAverage(indicatorHistVals)\n",
    "    #return max(indicatorHistValsSmoothed)\n",
    "    \n",
    "    #Return the count of the mode of all of the indicator functions. Returning the modal value would tell us how far through the database recording we are.\n",
    "    return mode(indicatorFunctions)[1][0]\n",
    "\n",
    "def computePairMatchingFunction(queryHashList, databaseHashList):\n",
    "    \"\"\"Find all hashes that are in both dictionaries, the corresponding offset times are calculated and we can make a histogram from these\n",
    "    Inputs: queryHashList (Dictionary) - contains \"\"\"\n",
    "    indicatorFunctions = np.empty((1,0), int)\n",
    "    \n",
    "    for queryHashKey in queryHashList:\n",
    "        if queryHashKey in databaseHashList:\n",
    "            #Add the time difference between matching pair of peaks.\n",
    "            indicatorFunctions = np.append(indicatorFunctions, queryHashList[queryHashKey] - databaseHashList[queryHashKey])\n",
    "    \n",
    "    if np.size(indicatorFunctions)==0:\n",
    "        return 0\n",
    "    return mode(indicatorFunctions)[1][0]\n",
    "\n",
    "#Test the compute matching function using the example from the lectures\n",
    "queryPrint = {2: np.array([0,2]), 3: np.array([1]), 4: np.array([1,2])}\n",
    "databasePrint = {1: np.array([0, 1, 3, 5]), 2: np.array([2, 4]), 3: np.array([0, 3, 5]), 4: np.array([3, 4])}\n",
    "\n",
    "score=computeMatchingFunction(queryPrint, databasePrint)\n",
    "#assert(score==5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMatchesForQuery(queryFileName, params, path_to_fingerprints=-1):\n",
    "    \"\"\"Computes the matching function for all of the files in the database and returns the one with the maximal value.\"\"\"\n",
    "    \n",
    "    #Get all of the variables out of the params\n",
    "    usePeakPairs = params['usePeakPairs']\n",
    "    \n",
    "    queryFingerprint = computeFingerprint(queryFileName, params)\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    \n",
    "    if path_to_fingerprints==-1:\n",
    "        path_to_fingerprints = 'database_recordings'\n",
    "    \n",
    "    for entry in os.scandir(path_to_fingerprints):\n",
    "        if (entry.path.endswith('pairs.npy') and usePeakPairs) or (entry.path.endswith('print.npy') and (usePeakPairs==False)):\n",
    "            databaseFingerprint = np.load(entry.path,allow_pickle=True)[()]\n",
    "            scoreForDBRecording = computeMatchingFunction(queryFingerprint, databaseFingerprint, usePeakPairs=usePeakPairs)\n",
    "            scores[entry.path] = scoreForDBRecording\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSongNamesFromFileNames(querySnippetFileName, databaseFileName):\n",
    "    \"\"\" Removes the gubbins around the file names. Used to identify whether the right song has been matched for a query.\"\"\"\n",
    "    #querySnippetFileName of the form 'query_recordings/classical.00003-snippet-x-x.wav'\n",
    "    \n",
    "    #Remove the 'query_recordings'\n",
    "    querySnippetFileName = querySnippetFileName[17:]\n",
    "    #Remove the '-snippet-x-x.wav'\n",
    "    if querySnippetFileName.startswith('classical'):\n",
    "        querySnippetFileName = querySnippetFileName[:15]\n",
    "    elif querySnippetFileName.startswith('jazz'):\n",
    "        querySnippetFileName = querySnippetFileName[:10]\n",
    "    elif querySnippetFileName.startswith('pop'):\n",
    "        querySnippetFileName = querySnippetFileName[:9]\n",
    "\n",
    "\n",
    "    #databaseFileName of the form 'database_recordings/classical.00052_print.npy'\n",
    "    # OR 'database_fingerprints/classical.00052_print.npy'\n",
    "    if databaseFileName.startswith('database_recordings'):\n",
    "        #Strip out the 'database_recordings'\n",
    "        databaseFileName = databaseFileName[20:]\n",
    "    else:\n",
    "        #Strip out the 'database_fingerprints'\n",
    "        databaseFileName = databaseFileName[22:]\n",
    "    #Strip out the '_print.npy'\n",
    "    if databaseFileName.startswith('classical'):\n",
    "        databaseFileName = databaseFileName[:15]\n",
    "    elif databaseFileName.startswith('jazz'):\n",
    "        databaseFileName = databaseFileName[:10]\n",
    "    elif databaseFileName.startswith('pop'):\n",
    "        databaseFileName = databaseFileName[:9]\n",
    "    \n",
    "    return querySnippetFileName, databaseFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNBestScoringRecordings(scores, N=1):\n",
    "    \"\"\"Simple search through the scores dictionary to find the best database recording given a query that has already been matched against the entire database.\n",
    "    \n",
    "    Inputs: scores (dictionary{database_recording_name : matchScore for a given query.})\n",
    "            N - How many best recordings to return.\n",
    "    \"\"\"\n",
    "    bestScoringRecordings = []\n",
    "    \n",
    "    for n in range(N):\n",
    "        maxScore = 0\n",
    "        bestScoringRecording = -1\n",
    "        for recordingName in scores:\n",
    "            if scores[recordingName] > maxScore:\n",
    "                bestScoringRecording = recordingName\n",
    "                maxScore = scores[recordingName]\n",
    "        bestScoringRecordings += [bestScoringRecording]\n",
    "        del scores[bestScoringRecording]\n",
    "    \n",
    "    return bestScoringRecordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def computeFingerprintsForEntireDataset(params, path_to_database='database_recordings', path_to_fingerprints=-1):\n",
    "    start = time.time()\n",
    "    for entry in os.scandir(path_to_database):\n",
    "        if entry.path[-4:] == '.wav':\n",
    "            #print('Processing', entry.name)\n",
    "            fingerprint = computeFingerprint(entry.path, params)\n",
    "            \n",
    "            #Save the fingerprint\n",
    "            if path_to_fingerprints == -1:\n",
    "                if params['usePeakPairs']:\n",
    "                    np.save(entry.path[:-4] + '_print_pairs.npy', fingerprint)\n",
    "                else:\n",
    "                    np.save(entry.path[:-4] + '_print.npy', fingerprint)\n",
    "            else:\n",
    "                if params['usePeakPairs']:\n",
    "                    np.save(path_to_fingerprints + '/' + entry.name[:-4] + '_print_pairs.npy', fingerprint)\n",
    "                else:\n",
    "                    np.save(path_to_fingerprints + '/' + entry.name[:-4] + '_print.npy', fingerprint)\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    fingerprintComputationTime = end-start\n",
    "    return fingerprintComputationTime\n",
    "\n",
    "def computeMatchesForEntireQueryDataset(params, path_to_queryset='query_recordings'):\n",
    "    numberOfMatches = 0\n",
    "    numberOfQueries = 0\n",
    "    start = time.time()\n",
    "    for entry in os.scandir(path_to_queryset):\n",
    "        if entry.path[-4:] == '.wav':\n",
    "            queryFile = entry.path\n",
    "            #try:\n",
    "            scores = findMatchesForQuery(queryFile, params)\n",
    "            bestScoringRecordings = getNBestScoringRecordings(scores, N=params['nBest'])\n",
    "\n",
    "            for recording in bestScoringRecordings:\n",
    "                querySong, matchedSong = getSongNamesFromFileNames(queryFile, recording)\n",
    "\n",
    "                if querySong == matchedSong:\n",
    "                    numberOfMatches += 1\n",
    "            #except:\n",
    "            #    print('Issue handling', queryFile)\n",
    "\n",
    "            numberOfQueries += 1\n",
    "            \n",
    "            #if numberOfQueries%2==0:\n",
    "            #    print('Total number of queries:', numberOfQueries)\n",
    "            #    print('Total number of matches:', numberOfMatches)\n",
    "    \n",
    "        #Early stop for grid search:\n",
    "        #if numberOfQueries >= 50:\n",
    "        #    break\n",
    "            \n",
    "    accuracy = 100*numberOfMatches/numberOfQueries\n",
    "    end = time.time()\n",
    "    matchingComputationTime = end-start\n",
    "    return matchingComputationTime, accuracy\n",
    "\n",
    "def processEntireDataset(params):\n",
    "    \"\"\"First compute all of the fingerprints for the entire dataset. Then find the best matches for all of the queries in the dataset.\"\"\"\n",
    "    \n",
    "    fingerprintComputationTime = computeFingerprintsForEntireDataset(params)\n",
    "    \n",
    "    matchingComputationTime, accuracy = computeMatchesForEntireQueryDataset(params)\n",
    "    \n",
    "    return fingerprintComputationTime, matchingComputationTime, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next cell not intended for marker use\n",
    "\n",
    "# Score the whole dataset using the best parameters from the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'usePeakPairs':False,\n",
    "    'hopLength':512, #Spectrogram hop length\n",
    "    'nFFT':1024, #Defines the number of bins in the stft or melspectrogram.\n",
    "    'maxPeaksPerSecond':15,\n",
    "    'peakDetectionMinDistance':10,\n",
    "    'fMax':4000, #only used for melspectrogram\n",
    "    'fanOut':10,\n",
    "    'targetZoneK':100,\n",
    "    'targetZoneT':15,\n",
    "    'nBest':5 #Returning the nBest best scoring fingerprints from the database.\n",
    "}\n",
    "fingerprintComputationTime, matchingComputationTime, accuracy = processEntireDataset(params)\n",
    "print('Pairs:', params['usePeakPairs'], 'nFFT:', params['nFFT'], 'maxPeaksPerSecond:', params['maxPeaksPerSecond'],\n",
    "      'PeakDetectionMinDistance:', params['peakDetectionMinDistance'],\n",
    "     'fanOut:', params['fanOut'], 'targetZoneK:', params['targetZoneK'], 'targetZoneT:', params['targetZoneT'])\n",
    "print('Time taken to compute fingerprints:', '{:04.1f}'.format(fingerprintComputationTime),\n",
    "      ' Time taken to compute matches:', '{:04.1f}'.format(matchingComputationTime),\n",
    "      ' Accuracy:', '{:04.1f}'.format(accuracy))\n",
    "params = {\n",
    "    'usePeakPairs':True,\n",
    "    'hopLength':512,\n",
    "    'nFFT':1024,\n",
    "    'maxPeaksPerSecond':15,\n",
    "    'peakDetectionMinDistance':10,\n",
    "    'fMax':4000,\n",
    "    'fanOut':10,\n",
    "    'targetZoneK':100,\n",
    "    'targetZoneT':15,\n",
    "    'nBest':5\n",
    "}\n",
    "fingerprintComputationTime, matchingComputationTime, accuracy = processEntireDataset(params)\n",
    "print('Pairs:', params['usePeakPairs'], 'nFFT:', params['nFFT'], 'maxPeaksPerSecond:', params['maxPeaksPerSecond'],\n",
    "      'PeakDetectionMinDistance:', params['peakDetectionMinDistance'],\n",
    "     'fanOut:', params['fanOut'], 'targetZoneK:', params['targetZoneK'], 'targetZoneT:', params['targetZoneT'])\n",
    "print('Time taken to compute fingerprints:', '{:04.1f}'.format(fingerprintComputationTime),\n",
    "      ' Time taken to compute matches:', '{:04.1f}'.format(matchingComputationTime),\n",
    "      ' Accuracy:', '{:04.1f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next cell not intended for marker use\n",
    "\n",
    "# Perform a Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Grid Search over some parameters\n",
    "#Peak Detection Parameters\n",
    "nffts = [1024] #[1024, 2048]\n",
    "peaksPerSeconds = [10] #[10, 20, 30] #Try 10 again?!\n",
    "peakMinDistances = [10] #[4, 10, 20]\n",
    "#Pairwise Matching Parameters\n",
    "fanOuts = [10] # [5, 10]\n",
    "targetZoneKs = [400] # [20, 50, 100]\n",
    "targetZoneTs = [15, 25, 35]\n",
    "for nFFT in nffts:\n",
    "    for maxPeaksPerSecond in peaksPerSeconds:\n",
    "        for peakDetectionMinDistance in peakMinDistances:\n",
    "            for fanOut in fanOuts:\n",
    "                for targetZoneK in targetZoneKs:\n",
    "                    for targetZoneT in targetZoneTs:\n",
    "                        params = {\n",
    "                            'usePeakPairs':True,\n",
    "                            'hopLength':512, #Spectrogram hop length\n",
    "                            'nFFT':nFFT, #Defines the number of bins in the stft or melspectrogram.\n",
    "                            'maxPeaksPerSecond':maxPeaksPerSecond, #\n",
    "                            'peakDetectionMinDistance':peakDetectionMinDistance,\n",
    "                            'fMax':4000, #only used for melspectrogram\n",
    "                            'fanOut':fanOut,\n",
    "                            'targetZoneK':targetZoneK,\n",
    "                            'targetZoneT':targetZoneT\n",
    "                        }\n",
    "\n",
    "                        fingerprintComputationTime, matchingComputationTime, accuracy = processEntireDataset(params)\n",
    "                        print('Pairs:', params['usePeakPairs'], 'nFFT:', params['nFFT'], 'maxPeaksPerSecond:', params['maxPeaksPerSecond'], 'PeakDetectionMinDistance:', params['peakDetectionMinDistance'],\n",
    "                             'fanOut:', params['fanOut'], 'targetZoneK:', params['targetZoneK'], 'targetZoneT:', params['targetZoneT'])\n",
    "                        print('Time taken to compute fingerprints:', '{:04.1f}'.format(fingerprintComputationTime), ' Time taken to compute matches:', '{:04.1f}'.format(matchingComputationTime), ' Accuracy:', '{:04.1f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next cell not intended for marker use.\n",
    "\n",
    "# Using line_profiler on one file\n",
    "\n",
    "Turns out most of the load is coming from loading the .npy files! And therefore the timing is mostly dependant on the database fingerprint size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'usePeakPairs':True,\n",
    "    'hopLength':512, \n",
    "    'nFFT':1024,\n",
    "    'maxPeaksPerSecond':15,\n",
    "    'peakDetectionMinDistance':10,\n",
    "    'fMax':4000, \n",
    "    'fanOut':10,\n",
    "    'targetZoneK':100,\n",
    "    'targetZoneT':15,\n",
    "    'nBest':3\n",
    "}\n",
    "queryFile = 'query_recordings/classical.00000-snippet-10-0.wav'\n",
    "entryname = 'classical.00000-snippet-10-0'\n",
    "queryFingerprint = computeFingerprint(queryFile, params)\n",
    "path_to_fingerprints = 'database_fingerprints'\n",
    "\n",
    "if params['usePeakPairs']:\n",
    "    np.save(path_to_fingerprints + '/' + entryname + '_print_pairs.npy', queryFingerprint)\n",
    "else:\n",
    "    np.save(path_to_fingerprints + '/' + entryname + '_print.npy', queryFingerprint)\n",
    "\n",
    "#actualFile = 'database_recordings/classical.00000_print.npy'\n",
    "#actualFingerprint = np.load(actualFile,allow_pickle=True)[()]\n",
    "%lprun -f findMatchesForQuery findMatchesForQuery(queryFile, params, path_to_fingerprints=path_to_fingerprints)\n",
    "#scores = computeMatchingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and cells required for assignment\n",
    "\n",
    "The above code was all useful for me in doing the assignment. The following two functions are for marking purposes. The cells following these functions have example uses of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fingerprintBuilder(pathToDatabase, pathToFingerprints):\n",
    "    \"\"\"Build fingerprints for the entire database of recordings.\n",
    "    \n",
    "    Inputs: pathToDatabase. The name of the folder (located in the same directory as this notebook) where the database recordings are held.\n",
    "            pathToFingerprints. The name of the folder (located in the same directory as this notebook) where the fingerprints will be saved to.\"\"\"\n",
    "    print('Building fingerprints for all files located in ' + pathToDatabase + ' and placing them in ' + pathToFingerprints)\n",
    "    params = {\n",
    "        'usePeakPairs':True,\n",
    "        'hopLength':512, \n",
    "        'nFFT':1024,\n",
    "        'maxPeaksPerSecond':15,\n",
    "        'peakDetectionMinDistance':10,\n",
    "        'fMax':4000, \n",
    "        'fanOut':10,\n",
    "        'targetZoneK':100,\n",
    "        'targetZoneT':15,\n",
    "        'nBest':3\n",
    "    }\n",
    "    computeFingerprintsForEntireDataset(params, path_to_database=pathToDatabase, path_to_fingerprints=pathToFingerprints)\n",
    "    print('Finished building fingerprints.')\n",
    "\n",
    "    \n",
    "def audioIdentification(pathToQueryset, pathToFingerprints, pathToOutput):\n",
    "    \"\"\"Compute matches for all possible query recordings in the query set.\n",
    "    \n",
    "    Inputs: pathToQueryset. The name of the folder (located in the same directory as this notebook)\n",
    "                                where the queryset recordings are held.\n",
    "            pathToFingerprints. The name of the folder (located in the same directory as this notebook)\n",
    "                                where the fingerprints (calculated using fingerprintBuilder) are held.\n",
    "            pathToOutput. The name of the .txt file (located in the same directory as this notebook)\n",
    "                            where the output from audio matching is saved to. \n",
    "            \"\"\"\n",
    "    print('Computing matches for all possible query recordings in ' + pathToQueryset + ' and placing the results in ' + pathToOutput)\n",
    "    params = {\n",
    "        'usePeakPairs':True,\n",
    "        'hopLength':512, \n",
    "        'nFFT':1024, \n",
    "        'maxPeaksPerSecond':15,\n",
    "        'peakDetectionMinDistance':10,\n",
    "        'fMax':4000,\n",
    "        'fanOut':10,\n",
    "        'targetZoneK':100,\n",
    "        'targetZoneT':15,\n",
    "        'nBest':3\n",
    "    }\n",
    "\n",
    "    with open(pathToOutput, 'w') as outputFile:\n",
    "        for entry in os.scandir(pathToQueryset):\n",
    "            if entry.path[-4:] == '.wav':\n",
    "                queryFile = entry.path\n",
    "                scores = findMatchesForQuery(queryFile, params, path_to_fingerprints=pathToFingerprints)\n",
    "                bestScoringRecordings = getNBestScoringRecordings(scores, N=params['nBest'])\n",
    "            \n",
    "                rowText = entry.name\n",
    "                for recording in bestScoringRecordings:\n",
    "                    querySong, matchedSong = getSongNamesFromFileNames(queryFile, recording)\n",
    "                    rowText = rowText + '\\t' + matchedSong + '.wav'\n",
    "                outputFile.write(rowText + '\\n')\n",
    "    print('Finished computing all matches.')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToDatabaseRecordings = 'database_recordings'\n",
    "pathToFingerprints = 'database_fingerprints'\n",
    "pathToQueryset = 'query_recordings'\n",
    "pathToOutput = 'output.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building fingerprints for all files located in database_recordings and placing them in database_fingerprints\n",
      "Finished building fingerprints.\n"
     ]
    }
   ],
   "source": [
    "fingerprintBuilder(pathToDatabaseRecordings, pathToFingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing matches for all possible query recordings in query_recordings and placing the results in output.txt\n",
      "Finished computing all matches.\n"
     ]
    }
   ],
   "source": [
    "audioIdentification(pathToQueryset, pathToFingerprints, pathToOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
